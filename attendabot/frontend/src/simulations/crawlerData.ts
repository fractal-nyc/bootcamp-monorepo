import type { ScalingFrame } from "./scalingTypes";

/**
 * Frames for the "Web Crawler â†’ LLM Training Pipeline" simulation.
 * Based on system design interview rubrics from Claude and Gemini.
 *
 * 9 frames total:
 *   1â€“5  Web Crawler (seed â†’ frontier â†’ workers â†’ dedup â†’ storage)
 *   6â€“9  LLM Pipeline (filter â†’ tokenize â†’ train â†’ checkpoint)
 */
export const crawlerFrames: ScalingFrame[] = [
  // â”€â”€ Frame 1: The Simplest Crawler â”€â”€
  {
    id: "single-fetch",
    title: "The Simplest Crawler",
    description:
      "Every web crawler starts the same way: pick a URL, download the page, extract the text and any links you find, save the content, then repeat. At small scale this is just a loop in a script. The challenge is doing this 5,000 times per second, 24 hours a day, across 10 billion pages â€” without hammering any single website, and without storing the same content twice. That loop is the entire design problem.",
    nodes: [
      {
        id: "web",
        label: "Web",
        icon: "ğŸŒ",
        color: "#6c8cff",
        x: 50,
        y: 12,
        detail: "10 billion pages",
      },
      {
        id: "crawler",
        label: "Crawler",
        icon: "ğŸ¤–",
        color: "#4ade80",
        x: 50,
        y: 50,
        detail: "Fetch + Parse + Extract",
      },
      {
        id: "queue",
        label: "URL Queue",
        icon: "ğŸ“‹",
        color: "#fb923c",
        x: 15,
        y: 50,
        detail: "What to crawl next",
      },
      {
        id: "storage",
        label: "Storage",
        icon: "ğŸ’¾",
        color: "#f472b6",
        x: 84,
        y: 50,
        detail: "Extracted text",
      },
    ],
    edges: [
      { from: "crawler", to: "web", label: "HTTP GET", offsetY: -5 },
      { from: "web", to: "crawler", label: "HTML", offsetY: 5 },
      { from: "crawler", to: "storage", label: "save text" },
      { from: "crawler", to: "queue", label: "new URLs", style: "dashed" },
      { from: "queue", to: "crawler", label: "next URL" },
    ],
  },

  // â”€â”€ Frame 2: URL Frontier & Politeness â”€â”€
  {
    id: "url-frontier",
    title: "URL Frontier & Politeness",
    description:
      "A naive queue won't scale. We need a URL Frontier: a two-queue architecture where \"front queues\" rank URLs by priority (PageRank, freshness, domain authority) and \"back queues\" enforce politeness â€” one queue per domain, rate-limited to 1 request per second. robots.txt rules are fetched and cached in Redis. DNS lookups are cached locally per worker (DNS resolution is a massive bottleneck at scale â€” every page fetch would stall otherwise). This design is how Common Crawl and Googlebot actually work.",
    changeSummary:
      "Replace the simple queue with a two-queue URL Frontier, add robots.txt caching and DNS caching for politeness and performance.",
    nodes: [
      {
        id: "seeds",
        label: "Seed URLs",
        icon: "ğŸŒ±",
        color: "#a78bfa",
        x: 10,
        y: 35,
        detail: "Trusted domains: Wikipedia,\narXiv, news sites...",
      },
      {
        id: "frontier",
        label: "URL Frontier",
        icon: "ğŸ—‚ï¸",
        color: "#fb923c",
        x: 38,
        y: 35,
        detail: "Front queues (priority)\n+ Back queues (per-domain)",
      },
      {
        id: "robots",
        label: "robots.txt Cache",
        icon: "ğŸ“œ",
        color: "#94a3b8",
        x: 20,
        y: 70,
        detail: "Redis â€” domain crawl rules\n& rate limits",
      },
      {
        id: "politeness",
        label: "Rate Limiter",
        icon: "âš–ï¸",
        color: "#94a3b8",
        x: 38,
        y: 70,
        detail: "1 req/sec per domain\nrespects Retry-After headers",
      },
      {
        id: "dns",
        label: "DNS Cache",
        icon: "ğŸ—ºï¸",
        color: "#94a3b8",
        x: 80,
        y: 70,
        detail: "Per-worker, 10-min TTL\neliminate DNS bottleneck",
      },
      {
        id: "workers",
        label: "Crawler Workers",
        icon: "ğŸ¤–",
        color: "#4ade80",
        x: 65,
        y: 35,
        detail: "~1,000 instances",
      },
      {
        id: "storage",
        label: "Storage",
        icon: "ğŸ’¾",
        color: "#f472b6",
        x: 88,
        y: 35,
        detail: "S3 / GCS",
      },
    ],
    edges: [
      { from: "seeds", to: "frontier", label: "bootstrap" },
      { from: "frontier", to: "workers", label: "batched URLs" },
      { from: "workers", to: "storage", label: "text + HTML" },
      {
        from: "workers",
        to: "frontier",
        label: "new URLs",
        style: "dashed",
        offsetY: 5,
      },
      {
        from: "robots",
        to: "frontier",
        label: "crawl rules",
        style: "dashed",
      },
      {
        from: "politeness",
        to: "frontier",
        label: "rate-limit ok",
        style: "dashed",
      },
      { from: "dns", to: "workers", label: "cached IPs", style: "dashed" },
    ],
    newNodeIds: ["seeds", "frontier", "robots", "politeness", "dns"],
  },

  // â”€â”€ Frame 3: Distributed at Scale â”€â”€
  {
    id: "distributed-workers",
    title: "1,000 Crawler Workers",
    description:
      "To crawl 10 billion pages in 30 days we need ~5,000 pages per second (10B Ã· 2.6M seconds). Each worker handles ~10 concurrent connections, so we need ~500 workers minimum â€” plan for 1,000 with headroom. The URL Frontier is partitioned by consistent hash of the domain: each partition is owned by one worker cluster. This eliminates cross-worker coordination for politeness (each cluster rate-limits its own domains) and scales linearly. Workers are stateless and disposable â€” if one crashes, its Kafka partition is reassigned automatically.",
    changeSummary:
      "Scale from one crawler to 1,000 stateless workers. Partition the URL Frontier by domain hash so each cluster owns its own set of domains.",
    nodes: [
      {
        id: "frontier",
        label: "URL Frontier",
        icon: "ğŸ—‚ï¸",
        color: "#fb923c",
        x: 50,
        y: 10,
        detail: "Partitioned by domain hash\n~1,000 Kafka partitions",
      },
      {
        id: "worker-a",
        label: "Worker Cluster A",
        icon: "ğŸ¤–",
        color: "#4ade80",
        x: 15,
        y: 50,
        detail: "~333 workers\nowns domains Aâ€“H",
        group: "worker-cluster",
      },
      {
        id: "worker-b",
        label: "Worker Cluster B",
        icon: "ğŸ¤–",
        color: "#4ade80",
        x: 50,
        y: 50,
        detail: "~334 workers\nowns domains Iâ€“Q",
        group: "worker-cluster",
      },
      {
        id: "worker-c",
        label: "Worker Cluster C",
        icon: "ğŸ¤–",
        color: "#4ade80",
        x: 85,
        y: 50,
        detail: "~333 workers\nowns domains Râ€“Z",
        group: "worker-cluster",
      },
      {
        id: "storage",
        label: "Object Storage",
        icon: "ğŸ’¾",
        color: "#f472b6",
        x: 50,
        y: 85,
        detail: "S3/GCS â€” 20 Gbps write\n~1.2 PB total (with compression)",
      },
    ],
    groups: [
      {
        id: "worker-cluster",
        label: "~1,000 Crawler Workers",
        x: 2,
        y: 35,
        width: 96,
        height: 28,
      },
    ],
    edges: [
      { from: "frontier", to: "worker-a", label: "Aâ€“H domains" },
      { from: "frontier", to: "worker-b" },
      { from: "frontier", to: "worker-c", label: "Râ€“Z domains" },
      { from: "worker-a", to: "storage" },
      { from: "worker-b", to: "storage" },
      { from: "worker-c", to: "storage" },
      {
        from: "worker-a",
        to: "frontier",
        label: "new URLs",
        style: "dashed",
      },
      {
        from: "worker-c",
        to: "frontier",
        label: "new URLs",
        style: "dashed",
      },
    ],
    newNodeIds: ["worker-a", "worker-b", "worker-c"],
  },

  // â”€â”€ Frame 4: Two-Level Deduplication â”€â”€
  {
    id: "deduplication",
    title: "Two-Level Deduplication",
    description:
      "The web is full of duplicates â€” mirror sites, scrapers, syndicated articles, dynamic pages with slightly different timestamps. Without deduplication, the LLM trains on recycled content and learns to repeat itself. We use two levels: (1) URL-level Bloom filter in Redis (~20 GB for 10B URLs at 0.01% false-positive rate) prevents re-crawling the same URL. (2) Content-level MinHash+LSH finds near-duplicates by computing 128 hash functions per document and grouping them by Locality-Sensitive Hashing bands. Documents with Jaccard similarity > 0.8 are near-duplicates. MinHash is preferred over SimHash here because it's tunable and handles documents of varying length â€” the same approach used by Common Crawl and FineWeb.",
    changeSummary:
      "Add a two-level deduplication layer: URL Bloom filter (fast, exact) + MinHash+LSH (fuzzy content-level).",
    nodes: [
      {
        id: "workers",
        label: "Crawler Workers",
        icon: "ğŸ¤–",
        color: "#4ade80",
        x: 50,
        y: 10,
        detail: "5,000 pages/sec",
      },
      {
        id: "bloom",
        label: "URL Bloom Filter",
        icon: "ğŸ”µ",
        color: "#6c8cff",
        x: 20,
        y: 42,
        detail: "Redis â€” 20 GB\n0.01% false-positive rate",
      },
      {
        id: "minhash",
        label: "MinHash + LSH",
        icon: "ğŸ”¢",
        color: "#a78bfa",
        x: 78,
        y: 42,
        detail: "128 hash functions\nJaccard sim > 0.8 = duplicate",
      },
      {
        id: "new-url",
        label: "New URL â†’ Frontier",
        icon: "âœ…",
        color: "#4ade80",
        x: 8,
        y: 78,
        detail: "Enqueue for crawling",
      },
      {
        id: "seen-url",
        label: "Already Seen",
        icon: "ğŸš«",
        color: "#ef4444",
        x: 36,
        y: 78,
        detail: "Discard â€” skip the fetch",
      },
      {
        id: "unique",
        label: "Unique Content",
        icon: "âœ…",
        color: "#4ade80",
        x: 64,
        y: 78,
        detail: "Save to storage",
      },
      {
        id: "near-dup",
        label: "Near-Duplicate",
        icon: "ğŸš«",
        color: "#ef4444",
        x: 90,
        y: 78,
        detail: "~30% of pages discarded",
      },
    ],
    edges: [
      { from: "workers", to: "bloom", label: "URL seen?" },
      { from: "workers", to: "minhash", label: "content hash" },
      { from: "bloom", to: "new-url", label: "no" },
      { from: "bloom", to: "seen-url", label: "yes", style: "dashed" },
      { from: "minhash", to: "unique", label: "unique" },
      {
        from: "minhash",
        to: "near-dup",
        label: "duplicate",
        style: "dashed",
      },
    ],
    newNodeIds: ["bloom", "minhash", "new-url", "seen-url", "unique", "near-dup"],
  },

  // â”€â”€ Frame 5: Storage Layer â€” The Handoff â”€â”€
  {
    id: "storage-layer",
    title: "Storage Layer â€” The Handoff",
    description:
      "The crawler produces four kinds of data, each stored differently. Raw HTML goes to S3 as WARC (Web ARChive) files â€” the same format used by Common Crawl and the Internet Archive. WARC bundles the HTTP request, response headers, and body together, preserving data provenance (when it was fetched, from what IP, with what headers). This is critical for data lineage in the LLM pipeline. Extracted clean text goes to S3 as JSONL (gzip-compressed), the lingua franca of ML data pipelines. URL metadata (crawl time, status, depth) goes to DynamoDB for fast per-URL lookups. Live crawl state (Bloom filter, rate-limiter counters, robots.txt cache) stays in Redis. The JSONL text is what gets handed off to Part 2.",
    changeSummary:
      "Wire up the full storage layer: WARC for raw HTML, JSONL for text, DynamoDB for metadata, Redis for live state.",
    nodes: [
      {
        id: "workers",
        label: "Crawler Workers",
        icon: "ğŸ¤–",
        color: "#4ade80",
        x: 50,
        y: 8,
        detail: "Processing complete",
      },
      {
        id: "warc",
        label: "WARC Archives",
        icon: "ğŸ“¦",
        color: "#6c8cff",
        x: 12,
        y: 45,
        detail: "S3 â€” Raw HTML\nWith HTTP headers + provenance",
        group: "outputs",
      },
      {
        id: "jsonl",
        label: "JSONL Text",
        icon: "ğŸ“",
        color: "#4ade80",
        x: 38,
        y: 45,
        detail: "S3 â€” Extracted Text\n~500 TB compressed (zstd)",
        group: "outputs",
      },
      {
        id: "metadata",
        label: "URL Metadata",
        icon: "ğŸ—ƒï¸",
        color: "#fb923c",
        x: 64,
        y: 45,
        detail: "DynamoDB â€” crawl time,\nstatus, depth, language",
        group: "outputs",
      },
      {
        id: "redis",
        label: "Crawl State",
        icon: "âš¡",
        color: "#f472b6",
        x: 88,
        y: 45,
        detail: "Redis â€” Bloom filter,\nrate-limit counters",
        group: "outputs",
      },
      {
        id: "pipeline",
        label: "â†’ LLM Pipeline",
        icon: "ğŸš€",
        color: "#a78bfa",
        x: 38,
        y: 82,
        detail: "500 TB of JSONL text\nready for data preparation",
      },
    ],
    groups: [
      {
        id: "outputs",
        label: "Web Crawler Output",
        x: 2,
        y: 30,
        width: 96,
        height: 30,
      },
    ],
    edges: [
      { from: "workers", to: "warc", label: "raw HTML" },
      { from: "workers", to: "jsonl", label: "clean text" },
      { from: "workers", to: "metadata", label: "URL stats" },
      { from: "workers", to: "redis", label: "state" },
      { from: "jsonl", to: "pipeline", label: "~500 TB text" },
    ],
    newNodeIds: ["warc", "jsonl", "metadata", "redis", "pipeline"],
  },

  // â”€â”€ Frame 6: Quality Filtering â€” 6 Stages â”€â”€
  {
    id: "quality-filter",
    title: "Quality Filtering â€” 6 Stages",
    description:
      "Raw web text is mostly garbage: ads, boilerplate, gibberish, spam, machine-translated content, and toxic text. Feeding it directly into an LLM degrades quality dramatically. We use a 6-stage cascade filter. The key insight is ordering by cost: cheap CPU heuristics first (removes ~60% of pages instantly), then heavier ML classifiers only on the survivors. Stage 1: Language ID (FastText, keep top 100 languages). Stage 2: Heuristics (word count, symbol ratio, repetition ratio, line length â€” cheap and fast). Stage 3: Perplexity filter (a small KenLM model flags incoherent text). Stage 4: Toxicity classifier (fine-tuned BERT). Stage 5: Educational value scorer (Llama-based judge, as used in Phi-1). Stage 6: Final content dedup at paragraph level. Only ~15% of raw pages survive to become training data.",
    changeSummary:
      "Begin the LLM data preparation phase. Apply a 6-stage quality filter cascade, removing ~85% of raw pages.",
    nodes: [
      {
        id: "raw",
        label: "Raw Text",
        icon: "ğŸ“¥",
        color: "#94a3b8",
        x: 8,
        y: 38,
        detail: "500 TB JSONL\nfrom crawler",
      },
      {
        id: "lang",
        label: "1. Language ID",
        icon: "ğŸ·ï¸",
        color: "#6c8cff",
        x: 26,
        y: 38,
        detail: "FastText classifier\nkeep top 100 languages",
      },
      {
        id: "heuristics",
        label: "2â€“3. Heuristics",
        icon: "ğŸ§¹",
        color: "#4ade80",
        x: 44,
        y: 38,
        detail: "Word count, symbol ratio,\nperplexity (KenLM)",
      },
      {
        id: "ml",
        label: "4â€“5. ML Classifiers",
        icon: "ğŸ§ ",
        color: "#fb923c",
        x: 62,
        y: 38,
        detail: "Toxicity (BERT)\n+ Edu score (Llama judge)",
      },
      {
        id: "dedup2",
        label: "6. Content Dedup",
        icon: "ğŸ”",
        color: "#a78bfa",
        x: 80,
        y: 38,
        detail: "MinHash+LSH\ndoc + paragraph level",
      },
      {
        id: "clean",
        label: "Clean Corpus",
        icon: "âœ…",
        color: "#4ade80",
        x: 80,
        y: 75,
        detail: "~75 TB (15% of raw)\nhigh-quality training text",
      },
      {
        id: "rejected",
        label: "Rejected (~85%)",
        icon: "ğŸ—‘ï¸",
        color: "#ef4444",
        x: 44,
        y: 75,
        detail: "Noise, spam, toxicity,\nduplicates â€” discarded",
      },
    ],
    edges: [
      { from: "raw", to: "lang" },
      { from: "lang", to: "heuristics", label: "âœ“ identified" },
      { from: "lang", to: "rejected", label: "âœ—", style: "dashed" },
      { from: "heuristics", to: "ml", label: "âœ“ quality" },
      { from: "heuristics", to: "rejected", label: "âœ—", style: "dashed" },
      { from: "ml", to: "dedup2", label: "âœ“ clean" },
      {
        from: "ml",
        to: "rejected",
        label: "âœ— toxic/spam",
        style: "dashed",
      },
      { from: "dedup2", to: "clean", label: "âœ“ unique" },
      {
        from: "dedup2",
        to: "rejected",
        label: "âœ— duplicate",
        style: "dashed",
      },
    ],
    newNodeIds: ["lang", "heuristics", "ml", "dedup2", "clean", "rejected"],
  },

  // â”€â”€ Frame 7: Tokenization & Data Mixing â”€â”€
  {
    id: "tokenization",
    title: "Tokenization & Data Mixing",
    description:
      "LLMs don't train on text â€” they train on integers. A BPE (Byte-Pair Encoding) tokenizer, trained separately on a representative corpus, maps text to a vocabulary of ~32,000â€“100,000 subword tokens (the same tokenizer is baked into the model and released with it). After tokenization, a sequence packer concatenates variable-length documents into fixed-length 4,096-token windows with special <|doc|> boundary tokens, ensuring no GPU cycles are wasted on padding. The packed sequences are then mixed with other data sources: web text gets ~70% weight, but code (10%), books (15%), and scientific papers (5%) are over-represented relative to their raw volume because they raise benchmark performance dramatically. Ablation studies using small \"canary\" models (3B params) validate these mix ratios before committing to a full training run.",
    changeSummary:
      "Tokenize the clean corpus with BPE, pack into fixed-length sequences, then mix with code/books/papers at calibrated ratios.",
    nodes: [
      {
        id: "corpus",
        label: "Clean Corpus",
        icon: "âœ…",
        color: "#4ade80",
        x: 10,
        y: 35,
        detail: "~75 TB text\nMultiple sources",
      },
      {
        id: "tokenizer",
        label: "BPE Tokenizer",
        icon: "ğŸ”¤",
        color: "#6c8cff",
        x: 35,
        y: 35,
        detail: "SentencePiece, 32k vocab\nBytes â†’ token IDs",
      },
      {
        id: "packer",
        label: "Sequence Packer",
        icon: "ğŸ“¦",
        color: "#a78bfa",
        x: 62,
        y: 35,
        detail: "4,096-token windows\n<|doc|> boundary tokens",
      },
      {
        id: "mixer",
        label: "Data Mixer",
        icon: "ğŸšï¸",
        color: "#fb923c",
        x: 35,
        y: 72,
        detail: "Web 70%, Books 15%\nCode 10%, Papers 5%",
      },
      {
        id: "tokens",
        label: "Training Tokens",
        icon: "ğŸ—ƒï¸",
        color: "#f472b6",
        x: 62,
        y: 72,
        detail: "~15 trillion tokens\nParquet shards on S3",
      },
      {
        id: "canary",
        label: "Canary Models",
        icon: "ğŸ¤",
        color: "#94a3b8",
        x: 88,
        y: 72,
        detail: "3B ablation models\nvalidate mix ratios",
      },
    ],
    edges: [
      { from: "corpus", to: "tokenizer", label: "raw text docs" },
      { from: "tokenizer", to: "packer", label: "token sequences" },
      { from: "packer", to: "mixer", label: "packed sequences" },
      { from: "mixer", to: "tokens", label: "shuffled batches" },
      { from: "mixer", to: "canary", label: "test mix", style: "dashed" },
      { from: "canary", to: "mixer", label: "adjust ratios", style: "dashed" },
    ],
    newNodeIds: ["tokenizer", "packer", "mixer", "tokens", "canary"],
  },

  // â”€â”€ Frame 8: Distributed Training â€” 3D Parallelism â”€â”€
  {
    id: "distributed-training",
    title: "Distributed Training â€” 3D Parallelism",
    description:
      "A 70B parameter model requires ~140 GB just to store the weights at bf16 precision â€” plus ~280 GB for optimizer states (Adam keeps momentum + variance for every parameter). It cannot fit on a single GPU (80 GB H100). Three dimensions of parallelism solve this: (1) Tensor Parallelism (TP=8): each transformer layer is split across 8 GPUs in the same node, connected via NVLink at 900 GB/s. (2) Pipeline Parallelism (PP=4): the 80 transformer layers are split into 4 sequential stages across 4 nodes, with micro-batching to keep all stages busy. (3) Data Parallelism (DP=64): the entire TP+PP grid is replicated 64 times, each processing a different slice of data. Gradients are averaged across DP replicas via NCCL AllReduce over InfiniBand. Total cluster: 4 (PP) Ã— 8 (TP) Ã— 64 (DP) = 2,048 H100 GPUs.",
    changeSummary:
      "Train the model on 2,048 H100 GPUs using 3D parallelism: Tensor Parallel within node, Pipeline Parallel across nodes, Data Parallel across replicas.",
    nodes: [
      {
        id: "loader",
        label: "Data Loader",
        icon: "ğŸ“¦",
        color: "#4ade80",
        x: 50,
        y: 8,
        detail: "Streams token batches from S3\n15 trillion tokens total",
      },
      {
        id: "pp1",
        label: "Layers 1â€“18",
        icon: "ğŸ–¥ï¸",
        color: "#6c8cff",
        x: 12,
        y: 50,
        detail: "Node 0, 8Ã—H100\nTP=8 via NVLink",
        group: "pp-group",
      },
      {
        id: "pp2",
        label: "Layers 19â€“36",
        icon: "ğŸ–¥ï¸",
        color: "#6c8cff",
        x: 36,
        y: 50,
        detail: "Node 1, 8Ã—H100\nTP=8 via NVLink",
        group: "pp-group",
      },
      {
        id: "pp3",
        label: "Layers 37â€“54",
        icon: "ğŸ–¥ï¸",
        color: "#6c8cff",
        x: 60,
        y: 50,
        detail: "Node 2, 8Ã—H100\nTP=8 via NVLink",
        group: "pp-group",
      },
      {
        id: "pp4",
        label: "Layers 55â€“70",
        icon: "ğŸ–¥ï¸",
        color: "#6c8cff",
        x: 84,
        y: 50,
        detail: "Node 3, 8Ã—H100\nTP=8 via NVLink",
        group: "pp-group",
      },
      {
        id: "allreduce",
        label: "Gradient AllReduce",
        icon: "ğŸ”„",
        color: "#fb923c",
        x: 50,
        y: 80,
        detail: "64 DP replicas\nNCCL over 400 Gbps InfiniBand",
      },
    ],
    groups: [
      {
        id: "pp-group",
        label: "Pipeline Parallel â€” 4 stages (PP=4) â€” one of 64 DP replicas",
        x: 2,
        y: 35,
        width: 96,
        height: 28,
      },
    ],
    edges: [
      { from: "loader", to: "pp1", label: "micro-batches" },
      { from: "pp1", to: "pp2", label: "activations" },
      { from: "pp2", to: "pp3", label: "activations" },
      { from: "pp3", to: "pp4", label: "activations" },
      { from: "pp4", to: "allreduce", label: "gradients" },
      {
        from: "allreduce",
        to: "pp1",
        label: "updated weights",
        style: "dashed",
      },
    ],
    newNodeIds: ["pp1", "pp2", "pp3", "pp4", "allreduce"],
  },

  // â”€â”€ Frame 9: Checkpointing & Evaluation â”€â”€
  {
    id: "checkpointing",
    title: "Checkpointing & Evaluation",
    description:
      "In a 2,048-GPU cluster, hardware failures are a daily occurrence â€” GPUs die, network links flap, NVMe drives fail. Without checkpointing, a failure at step 100,000 would mean restarting from scratch and losing weeks of compute. We checkpoint every 500 steps (~30 minutes), keeping the last 3 checkpoints in case the latest is corrupted. Crucially, checkpointing is asynchronous â€” GPUs continue training while the checkpoint (140 GB weights + 280 GB optimizer states) is streamed to S3 in the background. Evaluation runs in parallel on a held-out shard: standard benchmarks (MMLU, HellaSwag, ARC, HumanEval, MATH) are scored every 10K steps to track training dynamics and catch regressions. The trained model is the result of ~3 months of training on ~15 trillion tokens, producing a competitive open-weight model.",
    changeSummary:
      "Add async checkpointing for failure recovery and a parallel eval runner to track benchmark progress throughout training.",
    nodes: [
      {
        id: "loop",
        label: "Training Loop",
        icon: "ğŸƒ",
        color: "#6c8cff",
        x: 50,
        y: 10,
        detail: "70B params, 15T tokens\n~3 months on 2,048 H100s",
      },
      {
        id: "ckpt",
        label: "Async Checkpointer",
        icon: "ğŸ’¾",
        color: "#fb923c",
        x: 18,
        y: 45,
        detail: "Every 500 steps\nLast 3 checkpoints kept",
      },
      {
        id: "s3",
        label: "Blob Storage",
        icon: "ğŸ—ƒï¸",
        color: "#f472b6",
        x: 18,
        y: 80,
        detail: "S3 â€” weights (140 GB)\n+ optimizer states (280 GB)",
      },
      {
        id: "recovery",
        label: "Failure Recovery",
        icon: "ğŸ”„",
        color: "#4ade80",
        x: 50,
        y: 80,
        detail: "Resume from last checkpoint\nvs. restart from scratch",
      },
      {
        id: "eval",
        label: "Eval Runner",
        icon: "ğŸ“Š",
        color: "#a78bfa",
        x: 82,
        y: 45,
        detail: "Parallel to training\nevery 10K steps",
      },
      {
        id: "benchmarks",
        label: "Benchmarks",
        icon: "ğŸ“ˆ",
        color: "#4ade80",
        x: 82,
        y: 80,
        detail: "MMLU, HellaSwag, ARC\nHumanEval, MATH...",
      },
    ],
    edges: [
      { from: "loop", to: "ckpt", label: "model state" },
      { from: "ckpt", to: "s3", label: "async write", style: "dashed" },
      {
        from: "s3",
        to: "recovery",
        label: "on crash â†’ restore",
        style: "dashed",
      },
      {
        from: "recovery",
        to: "loop",
        label: "resume",
        style: "dashed",
      },
      { from: "loop", to: "eval", label: "checkpoint", style: "dashed" },
      { from: "eval", to: "benchmarks", label: "score" },
    ],
    newNodeIds: ["ckpt", "s3", "recovery", "eval", "benchmarks"],
  },
];
